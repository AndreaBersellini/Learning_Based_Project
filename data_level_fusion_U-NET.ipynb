{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4acd8-a424-4029-add8-7961c2be589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598e934-793c-4503-bf80-cbb0ed10410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "bs_train, bs_test = 8, 1\n",
    "epochs = 10\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3663392-0ea0-432b-a392-11d3f0687da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_files = []\n",
    "lidar_files = []\n",
    "depth_files = []\n",
    "\n",
    "camera_dir = glob('dataset/camera/*')\n",
    "lidar_dir = glob('dataset/lidar/*')\n",
    "depth_dir = glob('dataset/depth/*')\n",
    "\n",
    "for c in camera_dir:\n",
    "    camera_files.append(c)\n",
    "for l in lidar_dir:\n",
    "    lidar_files.append(l)\n",
    "for d in depth_dir:\n",
    "    depth_files.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        if np.array(img).ndim != 3:\n",
    "            return img.convert('L')\n",
    "        else:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceedd0-e356-4ed7-9e3f-8ac45b2c1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 3\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "for i in range(0, int(rows * cols / 3)):\n",
    "        r = random.randrange(0, 1000)\n",
    "        fig.add_subplot(rows, cols, i * 3 + 1)\n",
    "        camera_path = camera_files[r]\n",
    "        camera = pil_loader(camera_path)\n",
    "        plt.imshow(camera)\n",
    "        plt.axis('off')\n",
    "        fig.add_subplot(rows, cols, i * 3 + 2)\n",
    "        lidar_path = lidar_files[r]\n",
    "        lidar = pil_loader(lidar_path)\n",
    "        plt.imshow(lidar)\n",
    "        plt.axis('off')\n",
    "        fig.add_subplot(rows, cols, i * 3 + 3)\n",
    "        depth_path = depth_files[r]\n",
    "        depth = pil_loader(depth_path)\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c144fc-2fef-4a6a-b405-2e1783f992f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_split = len(camera_files)//2\n",
    "\n",
    "camera_train, lidar_train, depth_train = camera_files[ds_split:], lidar_files[ds_split:], depth_files[ds_split:]\n",
    "camera_test, lidar_test, depth_test = camera_files[:ds_split], lidar_files[:ds_split], depth_files[:ds_split]\n",
    "\n",
    "print(len(camera_train), len(camera_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image, zoom_factor):\n",
    "    width, height = image.size\n",
    "    new_width = int(width / zoom_factor)\n",
    "    new_height = int(height / zoom_factor)\n",
    "\n",
    "\n",
    "    left = (width - new_width) // 2\n",
    "    top = (height - new_height) // 2\n",
    "    right = left + new_width\n",
    "    bottom = top + new_height\n",
    "\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    image = image.resize((width, height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577ddeb-14d0-417e-84ad-a443fb78ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLFDataset(Dataset): # Data-Level Fusion Dataset\n",
    "    def __init__(self, camera_paths, lidar_paths, depth_paths, image_size : tuple, train = True):\n",
    "        self._camera_paths = camera_paths\n",
    "        self._lidar_paths = lidar_paths\n",
    "        self._depth_paths = depth_paths\n",
    "        self._image_size = image_size\n",
    "\n",
    "    def transform(self, camera, lidar, depth):\n",
    "        # 3 CHANNELS CONVERSION\n",
    "        rgb = transforms.Lambda(lambda img: img.convert(\"RGB\"))\n",
    "        camera = rgb(camera)\n",
    "        lidar = rgb(lidar)\n",
    "\n",
    "        # 1 CHANNEL CONVERSION\n",
    "        gray = transforms.Lambda(lambda img: img.convert(\"L\"))\n",
    "        depth = gray(depth)\n",
    "\n",
    "        # RESIZE\n",
    "        resize = transforms.Resize(self._image_size)\n",
    "        camera = resize(camera)\n",
    "        lidar = resize(lidar)\n",
    "        depth = resize(depth)\n",
    "\n",
    "        # ZOOM\n",
    "        #zoom_factor = calibration()\n",
    "        camera = zoom(camera, 1.8)\n",
    "        lidar = zoom(lidar, 0.7)\n",
    "\n",
    "        # HORIZONTAL FLIP\n",
    "        if random.random() > 0.5:\n",
    "            camera = TF.hflip(camera)\n",
    "            lidar = TF.hflip(lidar)\n",
    "            depth = TF.hflip(depth)\n",
    "\n",
    "        # CONVERT TO TENSOR\n",
    "        camera_tensor = TF.to_tensor(camera)\n",
    "        lidar_tensor = TF.to_tensor(lidar)\n",
    "        groundtruth = TF.to_tensor(depth)\n",
    "        \n",
    "        # FUSION\n",
    "        alpha = 0.6\n",
    "        image = camera_tensor * alpha + lidar_tensor * (1 - alpha)\n",
    "\n",
    "        return image, groundtruth\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        camera = Image.open(self._camera_paths[index])\n",
    "        lidar = Image.open(self._lidar_paths[index])\n",
    "        depth = Image.open(self._depth_paths[index])\n",
    "        x, y = self.transform(camera, lidar, depth)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._camera_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_loss(pred, target): # Dice loss function (NOT USED)\n",
    "    smooth = 1.\n",
    "\n",
    "    predf = pred.view(-1)\n",
    "    targetf = target.view(-1)\n",
    "    intersection = (predf * targetf).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (predf.sum() + targetf.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff87b-73af-4a8e-a590-bb2a6b8c3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_channels, output_channels):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # input dim 256x256\n",
    "        self.down_1 = conv_layer(3, 64) #128x128\n",
    "        self.down_2 = conv_layer(64, 128) #64x64\n",
    "        self.down_3 = conv_layer(128, 256) #32x32\n",
    "        self.down_4 = conv_layer(256, 512) #16x16\n",
    "        self.down_5 = conv_layer(512, 1024) #8x8\n",
    "        \n",
    "        self.up_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.up_conv_1 = conv_layer(1024, 512)\n",
    "        self.up_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.up_conv_2 = conv_layer(512, 256)\n",
    "        self.up_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.up_conv_3 = conv_layer(256, 128)\n",
    "        self.up_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.up_conv_4 = conv_layer(128, 64)\n",
    "        \n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "                \n",
    "    def forward(self, img):\n",
    "        x1 = self.down_1(img)\n",
    "        x2 = self.max_pool(x1)\n",
    "        x3 = self.down_2(x2)\n",
    "        x4 = self.max_pool(x3)\n",
    "        x5 = self.down_3(x4)\n",
    "        x6 = self.max_pool(x5)\n",
    "        x7 = self.down_4(x6)\n",
    "        x8 = self.max_pool(x7)\n",
    "        x9 = self.down_5(x8)\n",
    "        \n",
    "        x = self.up_1(x9)\n",
    "        x = self.up_conv_1(torch.cat([x, x7], 1))\n",
    "        x = self.up_2(x)\n",
    "        x = self.up_conv_2(torch.cat([x, x5], 1)) \n",
    "        x = self.up_3(x)\n",
    "        x = self.up_conv_3(torch.cat([x, x3], 1))\n",
    "        x = self.up_4(x)\n",
    "        x = self.up_conv_4(torch.cat([x, x1], 1))\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = self.output_activation(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9340ae-1cc3-4c3f-a366-7ce8d7c0980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "train_dataset = DLFDataset(camera_train, lidar_train, depth_train, (256, 256))\n",
    "test_dataset = DLFDataset(camera_test, lidar_test, depth_test, (256, 256))\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = bs_train, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = bs_test, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(di, dm) = next(iter(test_dataloader))\n",
    "di = di.to(device)\n",
    "dm = dm.to(device)\n",
    "dm = dm[0]\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.squeeze(di.cpu().numpy()).transpose(1,2,0))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow((dm.cpu().numpy()).transpose(1,2,0).squeeze(axis=2))\n",
    "plt.title('Original Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af22ec-911a-444f-8ad6-913db2cb68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model and optimizer\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, betas = (0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1d271-fb37-4b34-a06e-81cbee7e3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path where to save the model\n",
    "PATH = './models/unet_test.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd476b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayPrediction(model, img_pair):\n",
    "    (img, mask) = img_pair\n",
    "    img = img.to(device)\n",
    "    mask = mask.to(device)\n",
    "    mask = mask[0]\n",
    "    pred = model(img)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(np.squeeze(img.cpu().numpy()).transpose(1,2,0))\n",
    "    plt.title('Original Image')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2))\n",
    "    plt.title('Original Mask')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.squeeze(pred.cpu()))\n",
    "    plt.title('Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2df10d-c499-4c27-90b3-21a62f00b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, epochs):\n",
    "    \n",
    "    avg_train_losses = []\n",
    "    avg_test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = tqdm(enumerate(train_dataloader), total = len(train_dataloader), leave = False)\n",
    "        for batch, (images, targets) in loop:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = nn.MSELoss()\n",
    "            l = loss(pred, targets)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(l.item())\n",
    "            \n",
    "                        \n",
    "            with torch.no_grad():\n",
    "                if batch % 5 == 0:\n",
    "                    torch.save(model.state_dict(), PATH)\n",
    "                    model.eval()\n",
    "                    displayPrediction(model, next(iter(test_dataloader)))\n",
    "                    model.train()\n",
    "                    print(l.item())\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_batch, (test_images, test_targets) in enumerate(test_dataloader):\n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "                test_pred = model(test_images.detach())\n",
    "\n",
    "                test_loss = nn.MSELoss()\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            epoch_avg_train_loss = np.mean(train_losses)\n",
    "            epoch_avg_test_loss = np.mean(test_losses)\n",
    "            avg_train_losses.append(epoch_avg_train_loss)\n",
    "            avg_test_losses.append(epoch_avg_test_loss)\n",
    "\n",
    "            print_msg = (f'train_loss: {epoch_avg_train_loss:.5f} ' + f'valid_loss: {epoch_avg_test_loss:.5f}')\n",
    "            print(print_msg)\n",
    "\n",
    "    return  model, avg_train_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcfc7e-be29-488d-92eb-9e10f1e74951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model \n",
    "best_model, avg_train_losses, avg_val_losses = train(model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e68a4d-da0d-4448-b734-5cd4f0519d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './old_models/data_level_fusion.pth'\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06045e-20af-4b9e-ad6c-c10de925a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "(img, mask) = next(iter(test_dataloader))\n",
    "img = img.to(device)\n",
    "mask = mask.to(device)\n",
    "mask = mask[0]\n",
    "pred = model(img)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2))\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.squeeze(pred.cpu().detach().numpy()) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
